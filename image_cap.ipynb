{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "F9NEYgr156l_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Initialize CLIP model and processor\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Configure the Gemini API (replace with your API key)\n",
        "genai.configure(api_key=\"\")\n",
        "\n",
        "# Define Places365 and COCO labels\n",
        "places_labels = [\n",
        "    \"airport_terminal\", \"aquarium\", \"art_gallery\", \"badlands\", \"ballroom\", \"bamboo_forest\", \"banquet_hall\",\n",
        "    \"bar\", \"baseball_field\", \"basketball_court\", \"beach\", \"bedroom\", \"boardwalk\", \"boat_deck\", \"bookstore\",\n",
        "    \"botanical_garden\", \"bridge\", \"bus_interior\", \"campsite\", \"castle\", \"cemetery\", \"church_outdoor\",\n",
        "    \"classroom\", \"clothing_store\", \"coffee_shop\", \"concert_hall\", \"conference_room\", \"construction_site\",\n",
        "    \"corn_field\", \"corridor\", \"courtyard\", \"dining_room\", \"downtown\", \"fire_station\", \"forest_path\",\n",
        "    \"garden\", \"gymnasium\", \"harbor\", \"hospital_room\", \"hotel_room\", \"ice_cream_parlor\", \"kitchen\",\n",
        "    \"lake\", \"library\", \"living_room\", \"lobby\", \"market\", \"mountain_path\", \"museum\", \"nightclub\",\n",
        "    \"office\", \"park\", \"parking_garage\", \"pharmacy\", \"playground\", \"restaurant\", \"river\", \"schoolyard\",\n",
        "    \"shopping_mall\", \"stadium\", \"street\", \"subway_station\", \"swimming_pool\", \"temple\", \"theater\",\n",
        "    \"train_interior\", \"valley\", \"waterfall\", \"zoo\", \"bedroom\", \"street\"\n",
        "]\n",
        "\n",
        "coco_labels = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "    \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "    \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "    \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\",\n",
        "    \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "    \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"\n",
        "    Load an image from a URL or local file path.\n",
        "\n",
        "    :param image_path: URL or local path of the image\n",
        "    :return: Loaded PIL Image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if image_path.startswith(('http://', 'https://')):\n",
        "            # Load image from URL\n",
        "            response = requests.get(image_path)\n",
        "            image = Image.open(BytesIO(response.content))\n",
        "        else:\n",
        "            # Load image from local file path\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "        # Convert to RGB if needed\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image: {e}\")\n",
        "        raise\n",
        "\n",
        "def analyze_image_with_clip(image, top_places_count=1, top_objects_count=2):\n",
        "    \"\"\"\n",
        "    Analyze image using CLIP model to identify top places and objects.\n",
        "\n",
        "    :param image: PIL Image to analyze\n",
        "    :param top_places_count: Number of top places to return\n",
        "    :param top_objects_count: Number of top objects to return\n",
        "    :return: Tuple of top places and top objects\n",
        "    \"\"\"\n",
        "    # Combine all labels\n",
        "    all_labels = places_labels + coco_labels\n",
        "\n",
        "    # Prepare inputs for CLIP\n",
        "    inputs = processor(text=all_labels, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Run CLIP model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    # Get top predictions for places and objects\n",
        "    top_places_idx = torch.topk(probs[:, :len(places_labels)], top_places_count).indices.squeeze()\n",
        "    top_objects_idx = torch.topk(probs[:, len(places_labels):], top_objects_count).indices.squeeze()\n",
        "\n",
        "    # Convert indices to labels\n",
        "    top_places = [places_labels[idx] for idx in (top_places_idx.tolist() if top_places_count > 1 else [top_places_idx])]\n",
        "    top_objects = [coco_labels[idx] for idx in (top_objects_idx.tolist() if top_objects_count > 1 else [top_objects_idx])]\n",
        "\n",
        "    print(top_places)\n",
        "    print(top_objects)\n",
        "\n",
        "    return top_places, top_objects\n",
        "\n",
        "def generate_image_caption(image_path, context_word=None):\n",
        "    \"\"\"\n",
        "    Generate a detailed image caption using Gemini as a text LLM.\n",
        "\n",
        "    :param image_path: Path or URL of the image\n",
        "    :param context_word: Optional context word to influence the caption\n",
        "    :return: Generated caption\n",
        "    \"\"\"\n",
        "    # Load the image\n",
        "    image = load_image(image_path)\n",
        "\n",
        "    # Analyze image with CLIP\n",
        "    top_places, top_objects = analyze_image_with_clip(image)\n",
        "\n",
        "    # Initialize Gemini model for text generation\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "    # Prepare the prompt\n",
        "    prompt_parts = [\n",
        "        f\"The image contains a scene identified as a {', '.join(top_places)} with objects such as {', '.join(top_objects)}. \"\n",
        "    ]\n",
        "\n",
        "    # Add context word if provided\n",
        "    if context_word:\n",
        "        prompt_parts.append(f\"Include the context of '{context_word}' in the description. \")\n",
        "\n",
        "    prompt_parts.append(\"Provide a brief description of the image using just the provided words.\")\n",
        "\n",
        "    # Generate caption\n",
        "    response = model.generate_content('\\n'.join(prompt_parts))\n",
        "\n",
        "    return response.text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Replace with your image URL or local file path\n",
        "    image_url = \"test_images/1.jpg\"\n",
        "\n",
        "    # Optional: get context word from user\n",
        "    context_word = input(\"Enter a context word to influence the caption (or press Enter to skip): \").strip() or None\n",
        "\n",
        "    # Generate and print the caption\n",
        "    try:\n",
        "        caption = generate_image_caption(image_url, context_word)\n",
        "        print(\"\\nGenerated Caption:\")\n",
        "        print(caption)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "DkUCIXum6dJV",
        "outputId": "12f07d40-7021-4d8c-e4b7-71250f5e5e36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a context word to influence the caption (or press Enter to skip): bedroom \n",
            "['office']\n",
            "['laptop', 'cat']\n",
            "\n",
            "Generated Caption:\n",
            "A cat sits on a desk in a bedroom, next to a laptop.\n"
          ]
        }
      ]
    }
  ]
}
